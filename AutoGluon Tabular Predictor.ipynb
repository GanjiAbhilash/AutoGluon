{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d0223b36",
   "metadata": {},
   "source": [
    "<p style=\"padding: 10px; border: 1px solid black;\">\n",
    "<img src=\"./utils/MLU-NEW-logo.png\" alt=\"drawing\" width=\"400\"/> <br/>\n",
    "    \n",
    "# <a name=\"0\">MLU Workshop: Tuning Autogluon</a>\n",
    "   \n",
    "This notebook will demonstrate AutoGluon's TabularPredictor for solving machine learning tasks. \n",
    "\n",
    "In this notebook, you will test AutoGluon on a dataset comprising of products from Amazon's retail catalogue. The goal is to identify whether two products are similar or not.\n",
    "    \n",
    "> This is a __binary classification__ task. The label column indicates whether a given pair of products are similar or not <br>\n",
    "    \n",
    "### <a href=\"#Part-I---Tabular-Predictor\">Part I - Tabular Predictor</a>\n",
    "    \n",
    "1. <a href=\"#Loading-the-data\">Loading the Data</a>\n",
    "2. <a href=\"#Specifying-performance-metric-and-Hyperparameter-Options\">Specifying performance metric and Hyperparameter Options</a>   \n",
    "3. <a href=\"#Model-Ensembling\">Model Ensembling</a>\n",
    "4. <a href=\"#Train-and-Tune-the-Predictor\">Train and Tune the Predictor</a>\n",
    "5. <a href=\"#Saving-and-Loading-Models\">Saving and Loading Models</a>\n",
    "6. <a href=\"#Model-Inference\">Model Inference</a>\n",
    "    \n",
    "### <a href=\"#Part-II---Additional-Features\">Part II - Additional Features</a>\n",
    "1. <a href=\"#Feature-Importance\">Feature Importance</a>\n",
    "2. <a href=\"#Inference-Speed\">Inference Speed</a>\n",
    "3. <a href=\"#Excluding-Models\">Excluding Models</a>\n",
    "4. <a href=\"#Cleaning-up-Model-Artifacts\">Cleaning up Model Artifacts</a>\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24cfb325",
   "metadata": {},
   "source": [
    "__Jupiter notebooks environment__:\n",
    "\n",
    "* Jupiter notebooks allow creating and sharing documents that contain both code and rich text cells. If you are not familiar with Jupiter notebooks, read more [here](https://jupyter-notebook-beginner-guide.readthedocs.io/en/latest/what_is_jupyter.html). \n",
    "* This is a quick-start demo to bring you up to speed on coding and experimenting with machine learning. Move through the notebook __from top to bottom__. \n",
    "* Run each code cell to see its output. To run a cell, click within the cell and press __Shift+Enter__, or click __Run__ from the top of the page menu. \n",
    "* A `[*]` symbol next to the cell indicates the code is still running. A `[#]` symbol, where # is an integer, indicates it is finished.\n",
    "* Beware, __some code cells might take longer to run__, sometimes 5-10 minutes (depending on the task, installing packages and libraries, training models, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "156208e0",
   "metadata": {},
   "source": [
    "Let's start by loading some libraries and packages!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "39a5af77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Installing AutoGluon\n",
    "# !pip install -q autogluon\n",
    "# Load in libraries\n",
    "import pandas as pd\n",
    "# Importing the libraries needed to work with our Tabular dataset.\n",
    "from autogluon.tabular import TabularPredictor, TabularDataset\n",
    "# Additional library for tuning\n",
    "import autogluon.core as ag\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc420d74",
   "metadata": {},
   "source": [
    "# <a id=\"Part-I---Tabular-Predictor\">Part I - Tabular Predictor</a>\n",
    "\n",
    "Now that you know how to use the `TabularPredictor` using 3 lines of code, let us try to understand some of the processes and available configurations AutoGluon offers.\n",
    "\n",
    "(<a href=\"#0\">Go to top</a>)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd9bbfa1",
   "metadata": {},
   "source": [
    "## <a id=\"Loading-the-data\">Loading the data</a>\n",
    "Let's load the dataset into dataframes. For faster experimentation, let's sample a smaller dataset from the training dataset.\n",
    "\n",
    "(<a href=\"#0\">Go to top</a>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1875bb97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>list_price_value_1</th>\n",
       "      <th>product_type_1</th>\n",
       "      <th>item_name_1</th>\n",
       "      <th>product_description_1</th>\n",
       "      <th>bullet_point_1</th>\n",
       "      <th>brand_1</th>\n",
       "      <th>manufacturer_1</th>\n",
       "      <th>part_number_1</th>\n",
       "      <th>model_number_1</th>\n",
       "      <th>size_1</th>\n",
       "      <th>...</th>\n",
       "      <th>item_dimensions_width_2</th>\n",
       "      <th>item_dimensions_length_2</th>\n",
       "      <th>item_dimensions_height_2</th>\n",
       "      <th>list_price_currency_1</th>\n",
       "      <th>list_price_value_with_tax_1</th>\n",
       "      <th>list_price_currency_2</th>\n",
       "      <th>list_price_value_with_tax_2</th>\n",
       "      <th>imgID_1</th>\n",
       "      <th>imgID_2</th>\n",
       "      <th>ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4881</th>\n",
       "      <td>NaN</td>\n",
       "      <td>HEALTH_PERSONAL_CARE</td>\n",
       "      <td>ITA-MED Elastic Abdominal Binder - 4 Panels, U...</td>\n",
       "      <td>Provides inchBody Shapinginch effect and can a...</td>\n",
       "      <td>ITA-MED Elastic Abdominal Binder - 4 Panels, U...</td>\n",
       "      <td>ITA-MED</td>\n",
       "      <td>ITA-MED</td>\n",
       "      <td>4199575</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>417SCE9yYbL</td>\n",
       "      <td>411rKe02fsL</td>\n",
       "      <td>b023d986b46940f4b571a22b75d35306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10965</th>\n",
       "      <td>NaN</td>\n",
       "      <td>OFFICE_PRODUCTS</td>\n",
       "      <td>Vehicle Deal Jacket - Car Sales Envelope - Gol...</td>\n",
       "      <td>Quality 32# Deal Jackets for New or Used Vehic...</td>\n",
       "      <td>Color: Buff Quantity: 100</td>\n",
       "      <td>A Plus</td>\n",
       "      <td>A Plus</td>\n",
       "      <td>511</td>\n",
       "      <td>DSA-546B</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>413X0vOjSIL</td>\n",
       "      <td>413X0vOjSIL</td>\n",
       "      <td>fdab461a9bb54575800346c99dd8bbe7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11630</th>\n",
       "      <td>NaN</td>\n",
       "      <td>SHOES</td>\n",
       "      <td>Nike Air Terra Humara 18 Mens Mens Ao1545-004 ...</td>\n",
       "      <td>The Nike Air Terra Humara originally released ...</td>\n",
       "      <td>sku=ao1545-004-13</td>\n",
       "      <td>Nike</td>\n",
       "      <td>NaN</td>\n",
       "      <td>AO1545_004</td>\n",
       "      <td>AO1545</td>\n",
       "      <td>14.5 Women/13 Men</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>51YcDM9GJOL</td>\n",
       "      <td>51YcDM9GJOL</td>\n",
       "      <td>3e198620d32d4062b3b0024037249b08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18693</th>\n",
       "      <td>NaN</td>\n",
       "      <td>KITCHEN</td>\n",
       "      <td>Epica Automatic Electric Milk Frother and Heat...</td>\n",
       "      <td>Makes hot or cold milk froth for cappuccino or...</td>\n",
       "      <td>Epica Automatic Electric Milk Frother and Heat...</td>\n",
       "      <td>Epica</td>\n",
       "      <td>Epica</td>\n",
       "      <td>MU9EZ82</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>41vuRLxCHCL</td>\n",
       "      <td>41mM7kMpc2L</td>\n",
       "      <td>a9992b194c474e9b959275c7fd9ed005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17182</th>\n",
       "      <td>NaN</td>\n",
       "      <td>HOME_BED_AND_BATH</td>\n",
       "      <td>300 Thread Count Indian Finish 100%Egyptian Co...</td>\n",
       "      <td>100%Egyptian Cotton Sheets</td>\n",
       "      <td>Twin-XL Size ( flat sheet 66X96 Inch , fitted ...</td>\n",
       "      <td>Rahul Collection</td>\n",
       "      <td>Rahul Collection</td>\n",
       "      <td>Rahul Collection 4pcs-22155</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All Sizes</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>51ZVvHOg2GL</td>\n",
       "      <td>51ZVvHOg2GL</td>\n",
       "      <td>ce609cdc56a941149fe5d774937a6206</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 64 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       list_price_value_1        product_type_1  \\\n",
       "4881                  NaN  HEALTH_PERSONAL_CARE   \n",
       "10965                 NaN       OFFICE_PRODUCTS   \n",
       "11630                 NaN                 SHOES   \n",
       "18693                 NaN               KITCHEN   \n",
       "17182                 NaN     HOME_BED_AND_BATH   \n",
       "\n",
       "                                             item_name_1  \\\n",
       "4881   ITA-MED Elastic Abdominal Binder - 4 Panels, U...   \n",
       "10965  Vehicle Deal Jacket - Car Sales Envelope - Gol...   \n",
       "11630  Nike Air Terra Humara 18 Mens Mens Ao1545-004 ...   \n",
       "18693  Epica Automatic Electric Milk Frother and Heat...   \n",
       "17182  300 Thread Count Indian Finish 100%Egyptian Co...   \n",
       "\n",
       "                                   product_description_1  \\\n",
       "4881   Provides inchBody Shapinginch effect and can a...   \n",
       "10965  Quality 32# Deal Jackets for New or Used Vehic...   \n",
       "11630  The Nike Air Terra Humara originally released ...   \n",
       "18693  Makes hot or cold milk froth for cappuccino or...   \n",
       "17182                         100%Egyptian Cotton Sheets   \n",
       "\n",
       "                                          bullet_point_1           brand_1  \\\n",
       "4881   ITA-MED Elastic Abdominal Binder - 4 Panels, U...           ITA-MED   \n",
       "10965                          Color: Buff Quantity: 100            A Plus   \n",
       "11630                                  sku=ao1545-004-13              Nike   \n",
       "18693  Epica Automatic Electric Milk Frother and Heat...             Epica   \n",
       "17182  Twin-XL Size ( flat sheet 66X96 Inch , fitted ...  Rahul Collection   \n",
       "\n",
       "         manufacturer_1                part_number_1 model_number_1  \\\n",
       "4881            ITA-MED                      4199575            NaN   \n",
       "10965            A Plus                          511       DSA-546B   \n",
       "11630               NaN                   AO1545_004         AO1545   \n",
       "18693             Epica                      MU9EZ82            NaN   \n",
       "17182  Rahul Collection  Rahul Collection 4pcs-22155            NaN   \n",
       "\n",
       "                  size_1  ... item_dimensions_width_2  \\\n",
       "4881                 NaN  ...                     NaN   \n",
       "10965                NaN  ...                     NaN   \n",
       "11630  14.5 Women/13 Men  ...                     NaN   \n",
       "18693                NaN  ...                     NaN   \n",
       "17182          All Sizes  ...                     NaN   \n",
       "\n",
       "       item_dimensions_length_2 item_dimensions_height_2  \\\n",
       "4881                        NaN                      NaN   \n",
       "10965                       NaN                      NaN   \n",
       "11630                       NaN                      NaN   \n",
       "18693                       NaN                      NaN   \n",
       "17182                       NaN                      NaN   \n",
       "\n",
       "      list_price_currency_1 list_price_value_with_tax_1 list_price_currency_2  \\\n",
       "4881                    NaN                         NaN                   NaN   \n",
       "10965                   NaN                         NaN                   NaN   \n",
       "11630                   NaN                         NaN                   NaN   \n",
       "18693                   NaN                         NaN                   NaN   \n",
       "17182                   NaN                         NaN                   NaN   \n",
       "\n",
       "      list_price_value_with_tax_2      imgID_1      imgID_2  \\\n",
       "4881                          NaN  417SCE9yYbL  411rKe02fsL   \n",
       "10965                         NaN  413X0vOjSIL  413X0vOjSIL   \n",
       "11630                         NaN  51YcDM9GJOL  51YcDM9GJOL   \n",
       "18693                         NaN  41vuRLxCHCL  41mM7kMpc2L   \n",
       "17182                         NaN  51ZVvHOg2GL  51ZVvHOg2GL   \n",
       "\n",
       "                                     ID  \n",
       "4881   b023d986b46940f4b571a22b75d35306  \n",
       "10965  fdab461a9bb54575800346c99dd8bbe7  \n",
       "11630  3e198620d32d4062b3b0024037249b08  \n",
       "18693  a9992b194c474e9b959275c7fd9ed005  \n",
       "17182  ce609cdc56a941149fe5d774937a6206  \n",
       "\n",
       "[5 rows x 64 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the training dataset\n",
    "df_train = TabularDataset(data=\"../data/training.csv\")\n",
    "\n",
    "# Maintain a separate validation set for evaluation\n",
    "df_train, df_valid = train_test_split(df_train, test_size=1000, shuffle=True, random_state=0)\n",
    "\n",
    "# Load the test dataset\n",
    "df_test = TabularDataset(data=\"../data/mlu-leaderboard-test.csv\")\n",
    "\n",
    "# Subsample a subset of data for faster demo, try setting this to much larger values\n",
    "subsample_size = 5000\n",
    "\n",
    "df_train = df_train.sample(n=subsample_size, random_state=0)\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60b686b4",
   "metadata": {},
   "source": [
    "## <a id=\"Specifying-performance-metric-and-Hyperparameter-Options\">Specifying performance metric and Hyperparameter Options</a>\n",
    "\n",
    "### Specifying performance metric\n",
    "AutoGluon automatically infers the performance metric to optimize given the type of problem. However, it is possible to explicitly specify the evaluation metric as well. \n",
    "The full list of AutoGluon classification metrics can be found here:\n",
    "\n",
    "`'accuracy', 'balanced_accuracy', 'f1', 'f1_macro', 'f1_micro', 'f1_weighted', 'roc_auc', 'average_precision', 'precision', 'precision_macro', 'precision_micro', 'precision_weighted', 'recall', 'recall_macro', 'recall_micro', 'recall_weighted', 'log_loss', 'pac_score'`\n",
    "\n",
    "(<a href=\"#0\">Go to top</a>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2aa85604",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We specify eval-metric just for demo (unnecessary as it's the default)\n",
    "metric = \"accuracy\"\n",
    "\n",
    "# Train various models for ~5 min\n",
    "time_limit = 5 * 60"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d668d73a",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning\n",
    "Hyperparameter optimization improves model performance by finding the best combination of hyperparamter values. The choice of models and hyperparameters can be specified while calling the `fit()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "95ebaf9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Neural Net options\n",
    "# Specifies non-default hyperparameter values for neural network models\n",
    "nn_options = {\n",
    "    # number of training epochs (controls training time of NN models)\n",
    "    \"num_epochs\": 10,\n",
    "    # learning rate used in training (real-valued hyperparameter searched on log-scale)\n",
    "    \"learning_rate\": ag.space.Real(1e-4, 1e-2, default=5e-4, log=True),\n",
    "    # activation function used in NN (categorical hyperparameter, default = first entry)\n",
    "    \"activation\": ag.space.Categorical(\"relu\", \"softrelu\", \"tanh\"),\n",
    "    # each choice for categorical hyperparameter 'layers' corresponds to list of sizes for each NN layer to use\n",
    "    \"layers\": ag.space.Categorical([100], [1000], [200, 100], [300, 200, 100]),\n",
    "    # dropout probability (real-valued hyperparameter)\n",
    "    \"dropout_prob\": ag.space.Real(0.0, 0.5, default=0.1),\n",
    "}\n",
    "\n",
    "# Set GBM options\n",
    "# Specifies non-default hyperparameter values for lightGBM gradient boosted trees\n",
    "gbm_options = {\n",
    "    # number of boosting rounds (controls training time of GBM models)\n",
    "    \"num_boost_round\": 100,\n",
    "    # number of leaves in trees (integer hyperparameter)\n",
    "    \"num_leaves\": ag.space.Int(lower=26, upper=66, default=36),\n",
    "}\n",
    "\n",
    "# Add both NN and GBM options into a hyperparameter dictionary\n",
    "# hyperparameters of each model type\n",
    "# When these keys are missing from the hyperparameters dict, no models of that type are trained\n",
    "hyperparameters = {\n",
    "    \"GBM\": gbm_options,\n",
    "    \"NN\": nn_options,\n",
    "}\n",
    "\n",
    "# To tune hyperparameters using Bayesian optimization to find best combination of params\n",
    "search_strategy = \"auto\"\n",
    "\n",
    "# Number of trials for hyperparameters\n",
    "num_trials = 5\n",
    "\n",
    "# HPO is not performed unless hyperparameter_tune_kwargs is specified\n",
    "hyperparameter_tune_kwargs = {\n",
    "    \"num_trials\": num_trials,\n",
    "    \"scheduler\": \"local\",\n",
    "    \"searcher\": search_strategy,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0ba3a67",
   "metadata": {},
   "source": [
    "## <a id=\"Model-Ensembling\">Model Ensembling</a>\n",
    "Beyond hyperparameter-tuning with a correctly-specified evaluation metric, there are two other methods to boost predictive performance:\n",
    "- bagging and \n",
    "- stack-ensembling\n",
    "\n",
    "You’ll often see performance improve if you specify `num_bag_folds = 5-10`, `num_stack_levels = 1-3` in the call to `fit()`. Beware that doing this will increase training times and memory/disk usage.\n",
    "\n",
    "You should not provide `tuning_data` when stacking/bagging, and instead provide all your available data as `df_train` (which AutoGluon will split in more intelligent ways). Parameter `num_bag_sets` controls how many times the K-fold bagging process is repeated to further reduce variance (increasing this may further boost accuracy but will substantially increase training times, inference latency, and memory/disk usage). Rather than manually searching for good bagging/stacking values yourself, AutoGluon will automatically select good values for you if you specify `auto_stack` instead:\n",
    "\n",
    "Often stacking/bagging will produce superior accuracy than hyperparameter-tuning, but you may try combining both techniques (note: specifying `presets='best_quality'` in `fit()` simply sets `auto_stack=True`).\n",
    "\n",
    "(<a href=\"#0\">Go to top</a>)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "556b546f",
   "metadata": {},
   "source": [
    "## <a id=\"#Train-and-Tune-the-Predictor\">Train and Tune the Predictor</a>\n",
    "\n",
    "Now that we have specified hyperparamters for certain models and the tuning strategy, let us build an ensemble model that optimizes our selected evaluation metric, accuracy.\n",
    "\n",
    "(<a href=\"#0\">Go to top</a>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "18117f5a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"AutogluonModels/Tabular\"\n",
      "Warning: hyperparameter tuning is currently experimental and may cause the process to hang.\n",
      "Beginning AutoGluon training ... Time limit = 300s\n",
      "AutoGluon will save models to \"AutogluonModels/Tabular/\"\n",
      "AutoGluon Version:  0.3.1\n",
      "Train Data Rows:    5000\n",
      "Train Data Columns: 63\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'binary' (because only two unique label-values observed).\n",
      "\t2 unique label values:  [1, 0]\n",
      "\tIf 'binary' is not the correct problem_type, please manually specify the problem_type argument in fit() (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "NumExpr defaulting to 4 threads.\n",
      "Selected class <--> label mapping:  class 1 = 1, class 0 = 0\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    62627.32 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.7 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\t\tFitting CategoryFeatureGenerator...\n",
      "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
      "\t\tFitting TextSpecialFeatureGenerator...\n",
      "\t\t\tFitting BinnedFeatureGenerator...\n",
      "\t\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\t\tFitting TextNgramFeatureGenerator...\n",
      "\t\t\tFitting CountVectorizer for text features: ['item_name_1', 'product_description_1', 'bullet_point_1', 'generic_keyword_1', 'item_name_2', 'product_description_2', 'bullet_point_2', 'generic_keyword_2']\n",
      "\t\t\tCountVectorizer fit with vocabulary size = 4090\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tUseless Original Features (Count: 14): ['list_price_value_1', 'website_shipping_weight_1', 'list_price_value_2', 'website_shipping_weight_2', 'item_dimensions_width_1', 'item_dimensions_length_1', 'item_dimensions_height_1', 'item_dimensions_width_2', 'item_dimensions_length_2', 'item_dimensions_height_2', 'list_price_currency_1', 'list_price_value_with_tax_1', 'list_price_currency_2', 'list_price_value_with_tax_2']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tUnused Original Features (Count: 1): ['ID']\n",
      "\t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n",
      "\t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\t\t('object', []) : 1 | ['ID']\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', [])        : 10 | ['normalized_item_weight_1', 'number_of_items_1', 'case_pack_quantity_1', 'item_package_quantity_1', 'normalized_item_package_weight_1', ...]\n",
      "\t\t('object', [])       : 30 | ['product_type_1', 'brand_1', 'manufacturer_1', 'part_number_1', 'model_number_1', ...]\n",
      "\t\t('object', ['text']) :  8 | ['item_name_1', 'product_description_1', 'bullet_point_1', 'generic_keyword_1', 'item_name_2', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('category', [])                    :   30 | ['product_type_1', 'brand_1', 'manufacturer_1', 'part_number_1', 'model_number_1', ...]\n",
      "\t\t('category', ['text_as_category'])  :    8 | ['item_name_1', 'product_description_1', 'bullet_point_1', 'generic_keyword_1', 'item_name_2', ...]\n",
      "\t\t('float', [])                       :   10 | ['normalized_item_weight_1', 'number_of_items_1', 'case_pack_quantity_1', 'item_package_quantity_1', 'normalized_item_package_weight_1', ...]\n",
      "\t\t('int', ['binned', 'text_special']) :  186 | ['item_name_1.char_count', 'item_name_1.word_count', 'item_name_1.capital_ratio', 'item_name_1.lower_ratio', 'item_name_1.digit_ratio', ...]\n",
      "\t\t('int', ['text_ngram'])             : 4091 | ['__nlp__.00', '__nlp__.000', '__nlp__.01', '__nlp__.02', '__nlp__.03', ...]\n",
      "\t20.9s = Fit runtime\n",
      "\t48 features in original data used to generate 4325 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 42.58 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 21.55s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'accuracy'\n",
      "\tTo change this, specify the eval_metric argument of fit()\n",
      "AutoGluon will fit 2 stack levels (L1 to L2) ...\n",
      "Fitting 2 L1 models ...\n",
      "Hyperparameter tuning model: LightGBM_BAG_L1 ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5968b9adb3af42458137530e92abe0c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "/home/ec2-user/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "/home/ec2-user/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "/home/ec2-user/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "\tRan out of time, early stopping on iteration 92. Best iteration is:\n",
      "\t[50]\ttrain_set's binary_error: 0.15475\tvalid_set's binary_error: 0.296\n",
      "\tTime limit exceeded\n",
      "Fitted model: LightGBM_BAG_L1/T0 ...\n",
      "\t0.695\t = Validation score   (accuracy)\n",
      "\t2.04s\t = Training   runtime\n",
      "\t0.13s\t = Validation runtime\n",
      "Fitted model: LightGBM_BAG_L1/T1 ...\n",
      "\t0.682\t = Validation score   (accuracy)\n",
      "\t2.68s\t = Training   runtime\n",
      "\t0.14s\t = Validation runtime\n",
      "Fitted model: LightGBM_BAG_L1/T2 ...\n",
      "\t0.677\t = Validation score   (accuracy)\n",
      "\t3.18s\t = Training   runtime\n",
      "\t0.14s\t = Validation runtime\n",
      "Fitted model: LightGBM_BAG_L1/T3 ...\n",
      "\t0.704\t = Validation score   (accuracy)\n",
      "\t1.81s\t = Training   runtime\n",
      "\t0.13s\t = Validation runtime\n",
      "Hyperparameter tuning model: NeuralNetMXNet_BAG_L1 ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a5e37d0085347eb82e45521c60ea3c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022-08-04 15:30:35.611 ip-172-16-144-184:1351 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\n",
      "[2022-08-04 15:30:35.806 ip-172-16-144-184:1351 INFO profiler_config_parser.py:111] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tRan out of time, stopping training early. (Stopping on epoch 2)\n",
      "\tTime limit exceeded\n",
      "Fitted model: NeuralNetMXNet_BAG_L1/T0 ...\n",
      "\t0.594\t = Validation score   (accuracy)\n",
      "\t9.04s\t = Training   runtime\n",
      "\t0.53s\t = Validation runtime\n",
      "Fitting model: LightGBM_BAG_L1/T0 ... Training model for up to 130.94s of the 223.68s of remaining time.\n",
      "Attempting to fit model without HPO, but search space is provided. fit() will only consider default hyperparameter values from search space.\n",
      "/home/ec2-user/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "Attempting to fit model without HPO, but search space is provided. fit() will only consider default hyperparameter values from search space.\n",
      "/home/ec2-user/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "Attempting to fit model without HPO, but search space is provided. fit() will only consider default hyperparameter values from search space.\n",
      "/home/ec2-user/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "Attempting to fit model without HPO, but search space is provided. fit() will only consider default hyperparameter values from search space.\n",
      "/home/ec2-user/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "\t0.7012\t = Validation score   (accuracy)\n",
      "\t19.56s\t = Training   runtime\n",
      "\t0.75s\t = Validation runtime\n",
      "Fitting model: LightGBM_BAG_L1/T1 ... Training model for up to 112.6s of the 205.34s of remaining time.\n",
      "Attempting to fit model without HPO, but search space is provided. fit() will only consider default hyperparameter values from search space.\n",
      "/home/ec2-user/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "Attempting to fit model without HPO, but search space is provided. fit() will only consider default hyperparameter values from search space.\n",
      "/home/ec2-user/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "Attempting to fit model without HPO, but search space is provided. fit() will only consider default hyperparameter values from search space.\n",
      "/home/ec2-user/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "Attempting to fit model without HPO, but search space is provided. fit() will only consider default hyperparameter values from search space.\n",
      "/home/ec2-user/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "\t0.6986\t = Validation score   (accuracy)\n",
      "\t20.23s\t = Training   runtime\n",
      "\t0.75s\t = Validation runtime\n",
      "Fitting model: LightGBM_BAG_L1/T2 ... Training model for up to 94.24s of the 186.98s of remaining time.\n",
      "Attempting to fit model without HPO, but search space is provided. fit() will only consider default hyperparameter values from search space.\n",
      "/home/ec2-user/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "Attempting to fit model without HPO, but search space is provided. fit() will only consider default hyperparameter values from search space.\n",
      "/home/ec2-user/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "Attempting to fit model without HPO, but search space is provided. fit() will only consider default hyperparameter values from search space.\n",
      "/home/ec2-user/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "Attempting to fit model without HPO, but search space is provided. fit() will only consider default hyperparameter values from search space.\n",
      "/home/ec2-user/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "\t0.6976\t = Validation score   (accuracy)\n",
      "\t21.02s\t = Training   runtime\n",
      "\t0.76s\t = Validation runtime\n",
      "Fitting model: LightGBM_BAG_L1/T3 ... Training model for up to 75.57s of the 168.31s of remaining time.\n",
      "Attempting to fit model without HPO, but search space is provided. fit() will only consider default hyperparameter values from search space.\n",
      "/home/ec2-user/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "Attempting to fit model without HPO, but search space is provided. fit() will only consider default hyperparameter values from search space.\n",
      "/home/ec2-user/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "Attempting to fit model without HPO, but search space is provided. fit() will only consider default hyperparameter values from search space.\n",
      "/home/ec2-user/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "Attempting to fit model without HPO, but search space is provided. fit() will only consider default hyperparameter values from search space.\n",
      "/home/ec2-user/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "\t0.703\t = Validation score   (accuracy)\n",
      "\t19.36s\t = Training   runtime\n",
      "\t0.75s\t = Validation runtime\n",
      "Fitting model: NeuralNetMXNet_BAG_L1/T0 ... Training model for up to 57.19s of the 149.93s of remaining time.\n",
      "Attempting to fit model without HPO, but search space is provided. fit() will only consider default hyperparameter values from search space.\n",
      "\tRan out of time, stopping training early. (Stopping on epoch 3)\n",
      "Attempting to fit model without HPO, but search space is provided. fit() will only consider default hyperparameter values from search space.\n",
      "\tRan out of time, stopping training early. (Stopping on epoch 3)\n",
      "Attempting to fit model without HPO, but search space is provided. fit() will only consider default hyperparameter values from search space.\n",
      "\tRan out of time, stopping training early. (Stopping on epoch 3)\n",
      "Attempting to fit model without HPO, but search space is provided. fit() will only consider default hyperparameter values from search space.\n",
      "\tRan out of time, stopping training early. (Stopping on epoch 4)\n",
      "\t0.5954\t = Validation score   (accuracy)\n",
      "\t59.89s\t = Training   runtime\n",
      "\t3.12s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 278.45s of the 95.88s of remaining time.\n",
      "\t0.7034\t = Validation score   (accuracy)\n",
      "\t1.11s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting 2 L2 models ...\n",
      "Hyperparameter tuning model: LightGBM_BAG_L2 ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2eefabf10ed6462c96a84fc71d9244d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "\tTime limit exceeded\n",
      "Fitted model: LightGBM_BAG_L2/T0 ...\n",
      "\t0.703\t = Validation score   (accuracy)\n",
      "\t4.04s\t = Training   runtime\n",
      "\t0.13s\t = Validation runtime\n",
      "Hyperparameter tuning model: NeuralNetMXNet_BAG_L2 ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aaf1a74dcc9e4ac2b6cefe4ceb684065",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tRan out of time, stopping training early. (Stopping on epoch 1)\n",
      "\tTime limit exceeded\n",
      "Fitted model: NeuralNetMXNet_BAG_L2/T0 ...\n",
      "\t0.638\t = Validation score   (accuracy)\n",
      "\t5.29s\t = Training   runtime\n",
      "\t0.55s\t = Validation runtime\n",
      "Fitting model: LightGBM_BAG_L2/T0 ... Training model for up to 72.37s of the 71.76s of remaining time.\n",
      "Attempting to fit model without HPO, but search space is provided. fit() will only consider default hyperparameter values from search space.\n",
      "/home/ec2-user/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "Attempting to fit model without HPO, but search space is provided. fit() will only consider default hyperparameter values from search space.\n",
      "/home/ec2-user/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "Attempting to fit model without HPO, but search space is provided. fit() will only consider default hyperparameter values from search space.\n",
      "/home/ec2-user/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "Attempting to fit model without HPO, but search space is provided. fit() will only consider default hyperparameter values from search space.\n",
      "/home/ec2-user/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "\t0.6994\t = Validation score   (accuracy)\n",
      "\t30.01s\t = Training   runtime\n",
      "\t0.73s\t = Validation runtime\n",
      "Fitting model: NeuralNetMXNet_BAG_L2/T0 ... Training model for up to 45.13s of the 44.52s of remaining time.\n",
      "Attempting to fit model without HPO, but search space is provided. fit() will only consider default hyperparameter values from search space.\n",
      "\tRan out of time, stopping training early. (Stopping on epoch 2)\n",
      "Attempting to fit model without HPO, but search space is provided. fit() will only consider default hyperparameter values from search space.\n",
      "\tRan out of time, stopping training early. (Stopping on epoch 2)\n",
      "Attempting to fit model without HPO, but search space is provided. fit() will only consider default hyperparameter values from search space.\n",
      "\tRan out of time, stopping training early. (Stopping on epoch 2)\n",
      "Attempting to fit model without HPO, but search space is provided. fit() will only consider default hyperparameter values from search space.\n",
      "\tRan out of time, stopping training early. (Stopping on epoch 2)\n",
      "\t0.648\t = Validation score   (accuracy)\n",
      "\t45.4s\t = Training   runtime\n",
      "\t3.28s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L3 ... Training model for up to 278.45s of the 1.07s of remaining time.\n",
      "\t0.6994\t = Validation score   (accuracy)\n",
      "\t0.47s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 300.27s ...\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels/Tabular/\")\n"
     ]
    }
   ],
   "source": [
    "# Folder where to store trained models\n",
    "save_path = \"AutogluonModels/Tabular\"\n",
    "\n",
    "predictor = TabularPredictor(label=\"label\", eval_metric=metric, path=save_path).fit(\n",
    "    df_train,\n",
    "    time_limit=time_limit,\n",
    "    hyperparameters=hyperparameters,\n",
    "    hyperparameter_tune_kwargs=hyperparameter_tune_kwargs,\n",
    "    num_bag_folds=5,\n",
    "    num_bag_sets=1,\n",
    "    num_stack_levels=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01d9a7c2",
   "metadata": {},
   "source": [
    "Use the following to view a summary of what happened during the fit. Now this command will show details of the hyperparameter-tuning process for each type of model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8af0a0a0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Summary of fit() ***\n",
      "Estimated performance of each model:\n",
      "                      model  score_val  pred_time_val    fit_time  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n",
      "0       WeightedEnsemble_L2     0.7034       2.267676   61.724523                0.009824           1.114678            2       True          6\n",
      "1        LightGBM_BAG_L1/T3     0.7030       0.748693   19.364498                0.748693          19.364498            1       True          4\n",
      "2        LightGBM_BAG_L1/T0     0.7012       0.753334   19.556658                0.753334          19.556658            1       True          1\n",
      "3        LightGBM_BAG_L2/T0     0.6994       6.867795  170.063830                0.732568          30.010442            2       True          7\n",
      "4       WeightedEnsemble_L3     0.6994       6.877383  170.530808                0.009588           0.466978            3       True          9\n",
      "5        LightGBM_BAG_L1/T1     0.6986       0.746327   20.226154                0.746327          20.226154            1       True          2\n",
      "6        LightGBM_BAG_L1/T2     0.6976       0.762833   21.019193                0.762833          21.019193            1       True          3\n",
      "7  NeuralNetMXNet_BAG_L2/T0     0.6480       9.412838  185.450613                3.277610          45.397226            2       True          8\n",
      "8  NeuralNetMXNet_BAG_L1/T0     0.5954       3.124041   59.886885                3.124041          59.886885            1       True          5\n",
      "Number of models trained: 9\n",
      "Types of models trained:\n",
      "{'WeightedEnsembleModel', 'StackerEnsembleModel_LGB', 'StackerEnsembleModel_TabularNeuralNet'}\n",
      "Bagging used: True  (with 5 folds)\n",
      "Multi-layer stack-ensembling used: True  (with 3 levels)\n",
      "Feature Metadata (Processed):\n",
      "(raw dtype, special dtypes):\n",
      "('category', [])                    :   30 | ['product_type_1', 'brand_1', 'manufacturer_1', 'part_number_1', 'model_number_1', ...]\n",
      "('category', ['text_as_category'])  :    8 | ['item_name_1', 'product_description_1', 'bullet_point_1', 'generic_keyword_1', 'item_name_2', ...]\n",
      "('float', [])                       :   10 | ['normalized_item_weight_1', 'number_of_items_1', 'case_pack_quantity_1', 'item_package_quantity_1', 'normalized_item_package_weight_1', ...]\n",
      "('int', ['binned', 'text_special']) :  186 | ['item_name_1.char_count', 'item_name_1.word_count', 'item_name_1.capital_ratio', 'item_name_1.lower_ratio', 'item_name_1.digit_ratio', ...]\n",
      "('int', ['text_ngram'])             : 4091 | ['__nlp__.00', '__nlp__.000', '__nlp__.01', '__nlp__.02', '__nlp__.03', ...]\n",
      "Plot summary of models saved to file: AutogluonModels/Tabular/SummaryOfModels.html\n",
      "*** End of fit() summary ***\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'model_types': {'LightGBM_BAG_L1/T0': 'StackerEnsembleModel_LGB',\n",
       "  'LightGBM_BAG_L1/T1': 'StackerEnsembleModel_LGB',\n",
       "  'LightGBM_BAG_L1/T2': 'StackerEnsembleModel_LGB',\n",
       "  'LightGBM_BAG_L1/T3': 'StackerEnsembleModel_LGB',\n",
       "  'NeuralNetMXNet_BAG_L1/T0': 'StackerEnsembleModel_TabularNeuralNet',\n",
       "  'WeightedEnsemble_L2': 'WeightedEnsembleModel',\n",
       "  'LightGBM_BAG_L2/T0': 'StackerEnsembleModel_LGB',\n",
       "  'NeuralNetMXNet_BAG_L2/T0': 'StackerEnsembleModel_TabularNeuralNet',\n",
       "  'WeightedEnsemble_L3': 'WeightedEnsembleModel'},\n",
       " 'model_performance': {'LightGBM_BAG_L1/T0': 0.7012,\n",
       "  'LightGBM_BAG_L1/T1': 0.6986,\n",
       "  'LightGBM_BAG_L1/T2': 0.6976,\n",
       "  'LightGBM_BAG_L1/T3': 0.703,\n",
       "  'NeuralNetMXNet_BAG_L1/T0': 0.5954,\n",
       "  'WeightedEnsemble_L2': 0.7034,\n",
       "  'LightGBM_BAG_L2/T0': 0.6994,\n",
       "  'NeuralNetMXNet_BAG_L2/T0': 0.648,\n",
       "  'WeightedEnsemble_L3': 0.6994},\n",
       " 'model_best': 'WeightedEnsemble_L2',\n",
       " 'model_paths': {'LightGBM_BAG_L1/T0': 'AutogluonModels/Tabular/models/LightGBM_BAG_L1/T0/',\n",
       "  'LightGBM_BAG_L1/T1': 'AutogluonModels/Tabular/models/LightGBM_BAG_L1/T1/',\n",
       "  'LightGBM_BAG_L1/T2': 'AutogluonModels/Tabular/models/LightGBM_BAG_L1/T2/',\n",
       "  'LightGBM_BAG_L1/T3': 'AutogluonModels/Tabular/models/LightGBM_BAG_L1/T3/',\n",
       "  'NeuralNetMXNet_BAG_L1/T0': 'AutogluonModels/Tabular/models/NeuralNetMXNet_BAG_L1/T0/',\n",
       "  'WeightedEnsemble_L2': 'AutogluonModels/Tabular/models/WeightedEnsemble_L2/',\n",
       "  'LightGBM_BAG_L2/T0': 'AutogluonModels/Tabular/models/LightGBM_BAG_L2/T0/',\n",
       "  'NeuralNetMXNet_BAG_L2/T0': 'AutogluonModels/Tabular/models/NeuralNetMXNet_BAG_L2/T0/',\n",
       "  'WeightedEnsemble_L3': 'AutogluonModels/Tabular/models/WeightedEnsemble_L3/'},\n",
       " 'model_fit_times': {'LightGBM_BAG_L1/T0': 19.556658029556274,\n",
       "  'LightGBM_BAG_L1/T1': 20.22615385055542,\n",
       "  'LightGBM_BAG_L1/T2': 21.019192934036255,\n",
       "  'LightGBM_BAG_L1/T3': 19.364497900009155,\n",
       "  'NeuralNetMXNet_BAG_L1/T0': 59.886884927749634,\n",
       "  'WeightedEnsemble_L2': 1.114677906036377,\n",
       "  'LightGBM_BAG_L2/T0': 30.01044225692749,\n",
       "  'NeuralNetMXNet_BAG_L2/T0': 45.39722561836243,\n",
       "  'WeightedEnsemble_L3': 0.4669783115386963},\n",
       " 'model_pred_times': {'LightGBM_BAG_L1/T0': 0.7533340454101562,\n",
       "  'LightGBM_BAG_L1/T1': 0.7463271617889404,\n",
       "  'LightGBM_BAG_L1/T2': 0.7628326416015625,\n",
       "  'LightGBM_BAG_L1/T3': 0.748692512512207,\n",
       "  'NeuralNetMXNet_BAG_L1/T0': 3.1240413188934326,\n",
       "  'WeightedEnsemble_L2': 0.009823799133300781,\n",
       "  'LightGBM_BAG_L2/T0': 0.7325677871704102,\n",
       "  'NeuralNetMXNet_BAG_L2/T0': 3.2776100635528564,\n",
       "  'WeightedEnsemble_L3': 0.009587764739990234},\n",
       " 'num_bag_folds': 5,\n",
       " 'max_stack_level': 3,\n",
       " 'num_classes': 2,\n",
       " 'model_hyperparams': {'LightGBM_BAG_L1/T0': {'use_orig_features': True,\n",
       "   'max_base_models': 25,\n",
       "   'max_base_models_per_type': 5,\n",
       "   'save_bag_folds': True},\n",
       "  'LightGBM_BAG_L1/T1': {'use_orig_features': True,\n",
       "   'max_base_models': 25,\n",
       "   'max_base_models_per_type': 5,\n",
       "   'save_bag_folds': True},\n",
       "  'LightGBM_BAG_L1/T2': {'use_orig_features': True,\n",
       "   'max_base_models': 25,\n",
       "   'max_base_models_per_type': 5,\n",
       "   'save_bag_folds': True},\n",
       "  'LightGBM_BAG_L1/T3': {'use_orig_features': True,\n",
       "   'max_base_models': 25,\n",
       "   'max_base_models_per_type': 5,\n",
       "   'save_bag_folds': True},\n",
       "  'NeuralNetMXNet_BAG_L1/T0': {'use_orig_features': True,\n",
       "   'max_base_models': 25,\n",
       "   'max_base_models_per_type': 5,\n",
       "   'save_bag_folds': True},\n",
       "  'WeightedEnsemble_L2': {'use_orig_features': False,\n",
       "   'max_base_models': 25,\n",
       "   'max_base_models_per_type': 5,\n",
       "   'save_bag_folds': True},\n",
       "  'LightGBM_BAG_L2/T0': {'use_orig_features': True,\n",
       "   'max_base_models': 25,\n",
       "   'max_base_models_per_type': 5,\n",
       "   'save_bag_folds': True},\n",
       "  'NeuralNetMXNet_BAG_L2/T0': {'use_orig_features': True,\n",
       "   'max_base_models': 25,\n",
       "   'max_base_models_per_type': 5,\n",
       "   'save_bag_folds': True},\n",
       "  'WeightedEnsemble_L3': {'use_orig_features': False,\n",
       "   'max_base_models': 25,\n",
       "   'max_base_models_per_type': 5,\n",
       "   'save_bag_folds': True}},\n",
       " 'leaderboard':                       model  score_val  pred_time_val    fit_time  \\\n",
       " 0       WeightedEnsemble_L2     0.7034       2.267676   61.724523   \n",
       " 1        LightGBM_BAG_L1/T3     0.7030       0.748693   19.364498   \n",
       " 2        LightGBM_BAG_L1/T0     0.7012       0.753334   19.556658   \n",
       " 3        LightGBM_BAG_L2/T0     0.6994       6.867795  170.063830   \n",
       " 4       WeightedEnsemble_L3     0.6994       6.877383  170.530808   \n",
       " 5        LightGBM_BAG_L1/T1     0.6986       0.746327   20.226154   \n",
       " 6        LightGBM_BAG_L1/T2     0.6976       0.762833   21.019193   \n",
       " 7  NeuralNetMXNet_BAG_L2/T0     0.6480       9.412838  185.450613   \n",
       " 8  NeuralNetMXNet_BAG_L1/T0     0.5954       3.124041   59.886885   \n",
       " \n",
       "    pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  \\\n",
       " 0                0.009824           1.114678            2       True   \n",
       " 1                0.748693          19.364498            1       True   \n",
       " 2                0.753334          19.556658            1       True   \n",
       " 3                0.732568          30.010442            2       True   \n",
       " 4                0.009588           0.466978            3       True   \n",
       " 5                0.746327          20.226154            1       True   \n",
       " 6                0.762833          21.019193            1       True   \n",
       " 7                3.277610          45.397226            2       True   \n",
       " 8                3.124041          59.886885            1       True   \n",
       " \n",
       "    fit_order  \n",
       " 0          6  \n",
       " 1          4  \n",
       " 2          1  \n",
       " 3          7  \n",
       " 4          9  \n",
       " 5          2  \n",
       " 6          3  \n",
       " 7          8  \n",
       " 8          5  }"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor.fit_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32489e79",
   "metadata": {},
   "source": [
    "In the above example, the predictive performance may be poor because we are using few training data points and small ranges for hyperparameters to ensure quick run times. You can call `fit()` multiple times while modifying these settings to better understand how these choices affect performance outcomes. For example: you can increase `subsample_size` to train using a larger dataset, increase the `num_epochs` and `num_boost_round` hyperparameters, and increase the `time_limit` (which you should do for all code in these tutorials). To see more detailed output during the execution of `fit()`, you can also pass in the argument: `verbosity = 3`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42436096",
   "metadata": {},
   "source": [
    "Let us view the model summary generated by `fit_summary()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "60fac4b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "\n",
       "\n",
       "\n",
       "<!DOCTYPE html>\n",
       "<html lang=\"en\">\n",
       "  \n",
       "  <head>\n",
       "    \n",
       "      <meta charset=\"utf-8\">\n",
       "      <title>Models produced during fit()</title>\n",
       "      \n",
       "      \n",
       "        \n",
       "          \n",
       "        \n",
       "        \n",
       "          \n",
       "        <script type=\"text/javascript\" src=\"https://cdn.bokeh.org/bokeh/release/bokeh-2.3.3.min.js\" integrity=\"sha384-dM3QQsP+wXdHg42wTqW85BjZQdLNNIXqlPw/BgKoExPmTG7ZLML4EGqLMfqHT6ON\" crossorigin=\"anonymous\"></script>\n",
       "        <script type=\"text/javascript\">\n",
       "            Bokeh.set_log_level(\"info\");\n",
       "        </script>\n",
       "        \n",
       "      \n",
       "      \n",
       "    \n",
       "  </head>\n",
       "  \n",
       "  \n",
       "  <body>\n",
       "    \n",
       "      \n",
       "        \n",
       "          \n",
       "          \n",
       "            \n",
       "              <div class=\"bk-root\" id=\"ef34c3df-ddf4-44a3-967a-b1cd3ff3dd86\" data-root-id=\"1004\"></div>\n",
       "            \n",
       "          \n",
       "        \n",
       "      \n",
       "      \n",
       "        <script type=\"application/json\" id=\"1158\">\n",
       "          {\"21ab01eb-a16a-48e6-9b38-deee98fc41a6\":{\"defs\":[],\"roots\":{\"references\":[{\"attributes\":{},\"id\":\"1048\",\"type\":\"BasicTickFormatter\"},{\"attributes\":{},\"id\":\"1016\",\"type\":\"BasicTicker\"},{\"attributes\":{},\"id\":\"1046\",\"type\":\"AllLabels\"},{\"attributes\":{\"data_source\":{\"id\":\"1003\"},\"glyph\":{\"id\":\"1040\"},\"hover_glyph\":null,\"muted_glyph\":null,\"nonselection_glyph\":{\"id\":\"1041\"},\"view\":{\"id\":\"1043\"}},\"id\":\"1042\",\"type\":\"GlyphRenderer\"},{\"attributes\":{\"data\":{\"hyperparameters\":[\"use_orig_features: True&lt;br&gt;max_base_models: 25&lt;br&gt;max_base_models_per_type: 5&lt;br&gt;save_bag_folds: True\",\"use_orig_features: True&lt;br&gt;max_base_models: 25&lt;br&gt;max_base_models_per_type: 5&lt;br&gt;save_bag_folds: True\",\"use_orig_features: True&lt;br&gt;max_base_models: 25&lt;br&gt;max_base_models_per_type: 5&lt;br&gt;save_bag_folds: True\",\"use_orig_features: True&lt;br&gt;max_base_models: 25&lt;br&gt;max_base_models_per_type: 5&lt;br&gt;save_bag_folds: True\",\"use_orig_features: True&lt;br&gt;max_base_models: 25&lt;br&gt;max_base_models_per_type: 5&lt;br&gt;save_bag_folds: True\",\"use_orig_features: False&lt;br&gt;max_base_models: 25&lt;br&gt;max_base_models_per_type: 5&lt;br&gt;save_bag_folds: True\",\"use_orig_features: True&lt;br&gt;max_base_models: 25&lt;br&gt;max_base_models_per_type: 5&lt;br&gt;save_bag_folds: True\",\"use_orig_features: True&lt;br&gt;max_base_models: 25&lt;br&gt;max_base_models_per_type: 5&lt;br&gt;save_bag_folds: True\",\"use_orig_features: False&lt;br&gt;max_base_models: 25&lt;br&gt;max_base_models_per_type: 5&lt;br&gt;save_bag_folds: True\"],\"inference_latency\":[0.7533340454101562,0.7463271617889404,0.7628326416015625,0.748692512512207,3.1240413188934326,2.2676761150360107,6.867795467376709,9.412837743759155,6.877383232116699],\"model\":[\"LightGBM_BAG_L1/T0\",\"LightGBM_BAG_L1/T1\",\"LightGBM_BAG_L1/T2\",\"LightGBM_BAG_L1/T3\",\"NeuralNetMXNet_BAG_L1/T0\",\"WeightedEnsemble_L2\",\"LightGBM_BAG_L2/T0\",\"NeuralNetMXNet_BAG_L2/T0\",\"WeightedEnsemble_L3\"],\"model_type\":[\"StackerEnsembleModel_LGB\",\"StackerEnsembleModel_LGB\",\"StackerEnsembleModel_LGB\",\"StackerEnsembleModel_LGB\",\"StackerEnsembleModel_TabularNeuralNet\",\"WeightedEnsembleModel\",\"StackerEnsembleModel_LGB\",\"StackerEnsembleModel_TabularNeuralNet\",\"WeightedEnsembleModel\"],\"performance\":[0.7012,0.6986,0.6976,0.703,0.5954,0.7034,0.6994,0.648,0.6994],\"training_time\":[19.556658029556274,20.22615385055542,21.019192934036255,19.364497900009155,59.886884927749634,61.72452259063721,170.06382989883423,185.45061326026917,170.53080821037292]},\"selected\":{\"id\":\"1052\"},\"selection_policy\":{\"id\":\"1051\"}},\"id\":\"1003\",\"type\":\"ColumnDataSource\"},{\"attributes\":{},\"id\":\"1025\",\"type\":\"WheelZoomTool\"},{\"attributes\":{\"axis_label\":\"inference_latency\",\"formatter\":{\"id\":\"1045\"},\"major_label_policy\":{\"id\":\"1046\"},\"ticker\":{\"id\":\"1016\"}},\"id\":\"1015\",\"type\":\"LinearAxis\"},{\"attributes\":{},\"id\":\"1013\",\"type\":\"LinearScale\"},{\"attributes\":{},\"id\":\"1011\",\"type\":\"LinearScale\"},{\"attributes\":{\"bottom_units\":\"screen\",\"fill_alpha\":0.5,\"fill_color\":\"lightgrey\",\"left_units\":\"screen\",\"level\":\"overlay\",\"line_alpha\":1.0,\"line_color\":\"black\",\"line_dash\":[4,4],\"line_width\":2,\"right_units\":\"screen\",\"syncable\":false,\"top_units\":\"screen\"},\"id\":\"1030\",\"type\":\"BoxAnnotation\"},{\"attributes\":{\"fill_alpha\":{\"value\":0.5},\"fill_color\":{\"field\":\"model_type\",\"transform\":{\"id\":\"1002\"}},\"line_alpha\":{\"value\":0.5},\"line_color\":{\"field\":\"model_type\",\"transform\":{\"id\":\"1002\"}},\"size\":{\"value\":20},\"x\":{\"field\":\"inference_latency\"},\"y\":{\"field\":\"performance\"}},\"id\":\"1040\",\"type\":\"Circle\"},{\"attributes\":{\"axis_label\":\"performance\",\"formatter\":{\"id\":\"1048\"},\"major_label_policy\":{\"id\":\"1049\"},\"ticker\":{\"id\":\"1020\"}},\"id\":\"1019\",\"type\":\"LinearAxis\"},{\"attributes\":{},\"id\":\"1009\",\"type\":\"DataRange1d\"},{\"attributes\":{\"index\":0,\"label\":{\"value\":\"StackerEnsembleModel_LGB\"},\"renderers\":[{\"id\":\"1042\"}]},\"id\":\"1055\",\"type\":\"LegendItem\"},{\"attributes\":{\"fill_alpha\":{\"value\":0.1},\"fill_color\":{\"field\":\"model_type\",\"transform\":{\"id\":\"1002\"}},\"line_alpha\":{\"value\":0.1},\"line_color\":{\"field\":\"model_type\",\"transform\":{\"id\":\"1002\"}},\"size\":{\"value\":20},\"x\":{\"field\":\"inference_latency\"},\"y\":{\"field\":\"performance\"}},\"id\":\"1041\",\"type\":\"Circle\"},{\"attributes\":{},\"id\":\"1052\",\"type\":\"Selection\"},{\"attributes\":{\"index\":5,\"label\":{\"value\":\"WeightedEnsembleModel\"},\"renderers\":[{\"id\":\"1042\"}]},\"id\":\"1054\",\"type\":\"LegendItem\"},{\"attributes\":{},\"id\":\"1007\",\"type\":\"DataRange1d\"},{\"attributes\":{\"items\":[{\"id\":\"1054\"},{\"id\":\"1055\"},{\"id\":\"1056\"}],\"location\":[0,0]},\"id\":\"1057\",\"type\":\"Legend\"},{\"attributes\":{},\"id\":\"1020\",\"type\":\"BasicTicker\"},{\"attributes\":{},\"id\":\"1049\",\"type\":\"AllLabels\"},{\"attributes\":{\"text\":\"Models produced during fit()\"},\"id\":\"1005\",\"type\":\"Title\"},{\"attributes\":{\"active_multi\":null,\"tools\":[{\"id\":\"1023\"},{\"id\":\"1024\"},{\"id\":\"1025\"},{\"id\":\"1026\"},{\"id\":\"1027\"},{\"id\":\"1028\"},{\"id\":\"1029\"}]},\"id\":\"1031\",\"type\":\"Toolbar\"},{\"attributes\":{\"source\":{\"id\":\"1003\"}},\"id\":\"1043\",\"type\":\"CDSView\"},{\"attributes\":{},\"id\":\"1024\",\"type\":\"PanTool\"},{\"attributes\":{},\"id\":\"1051\",\"type\":\"UnionRenderers\"},{\"attributes\":{\"index\":4,\"label\":{\"value\":\"StackerEnsembleModel_TabularNeuralNet\"},\"renderers\":[{\"id\":\"1042\"}]},\"id\":\"1056\",\"type\":\"LegendItem\"},{\"attributes\":{},\"id\":\"1023\",\"type\":\"CrosshairTool\"},{\"attributes\":{\"axis\":{\"id\":\"1019\"},\"dimension\":1,\"ticker\":null},\"id\":\"1022\",\"type\":\"Grid\"},{\"attributes\":{},\"id\":\"1029\",\"type\":\"SaveTool\"},{\"attributes\":{\"callback\":null,\"tooltips\":[[\"performance\",\"@performance{safe}\"],[\"model\",\"@model{safe}\"],[\"model_type\",\"@model_type{safe}\"],[\"hyperparameters\",\"@hyperparameters{safe}\"],[\"inference_latency\",\"@inference_latency{safe}\"],[\"training_time\",\"@training_time{safe}\"]]},\"id\":\"1028\",\"type\":\"HoverTool\"},{\"attributes\":{},\"id\":\"1027\",\"type\":\"ResetTool\"},{\"attributes\":{},\"id\":\"1045\",\"type\":\"BasicTickFormatter\"},{\"attributes\":{\"overlay\":{\"id\":\"1030\"}},\"id\":\"1026\",\"type\":\"BoxZoomTool\"},{\"attributes\":{\"factors\":[\"WeightedEnsembleModel\",\"StackerEnsembleModel_LGB\",\"StackerEnsembleModel_TabularNeuralNet\"],\"palette\":[\"#1f77b4\",\"#ff7f0e\",\"#2ca02c\"]},\"id\":\"1002\",\"type\":\"CategoricalColorMapper\"},{\"attributes\":{\"axis\":{\"id\":\"1015\"},\"ticker\":null},\"id\":\"1018\",\"type\":\"Grid\"},{\"attributes\":{\"below\":[{\"id\":\"1015\"}],\"center\":[{\"id\":\"1018\"},{\"id\":\"1022\"}],\"left\":[{\"id\":\"1019\"}],\"renderers\":[{\"id\":\"1042\"}],\"right\":[{\"id\":\"1057\"}],\"title\":{\"id\":\"1005\"},\"toolbar\":{\"id\":\"1031\"},\"x_range\":{\"id\":\"1007\"},\"x_scale\":{\"id\":\"1011\"},\"y_range\":{\"id\":\"1009\"},\"y_scale\":{\"id\":\"1013\"}},\"id\":\"1004\",\"subtype\":\"Figure\",\"type\":\"Plot\"}],\"root_ids\":[\"1004\"]},\"title\":\"Bokeh Application\",\"version\":\"2.3.3\"}}\n",
       "        </script>\n",
       "        <script type=\"text/javascript\">\n",
       "          (function() {\n",
       "            var fn = function() {\n",
       "              Bokeh.safely(function() {\n",
       "                (function(root) {\n",
       "                  function embed_document(root) {\n",
       "                    \n",
       "                  var docs_json = document.getElementById('1158').textContent;\n",
       "                  var render_items = [{\"docid\":\"21ab01eb-a16a-48e6-9b38-deee98fc41a6\",\"root_ids\":[\"1004\"],\"roots\":{\"1004\":\"ef34c3df-ddf4-44a3-967a-b1cd3ff3dd86\"}}];\n",
       "                  root.Bokeh.embed.embed_items(docs_json, render_items);\n",
       "                \n",
       "                  }\n",
       "                  if (root.Bokeh !== undefined) {\n",
       "                    embed_document(root);\n",
       "                  } else {\n",
       "                    var attempts = 0;\n",
       "                    var timer = setInterval(function(root) {\n",
       "                      if (root.Bokeh !== undefined) {\n",
       "                        clearInterval(timer);\n",
       "                        embed_document(root);\n",
       "                      } else {\n",
       "                        attempts++;\n",
       "                        if (attempts > 100) {\n",
       "                          clearInterval(timer);\n",
       "                          console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\");\n",
       "                        }\n",
       "                      }\n",
       "                    }, 10, root)\n",
       "                  }\n",
       "                })(window);\n",
       "              });\n",
       "            };\n",
       "            if (document.readyState != \"loading\") fn();\n",
       "            else document.addEventListener(\"DOMContentLoaded\", fn);\n",
       "          })();\n",
       "        </script>\n",
       "    \n",
       "  </body>\n",
       "  \n",
       "</html>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "\n",
    "display(HTML(filename=\"AutogluonModels/Tabular/SummaryOfModels.html\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "531565f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>score_val</th>\n",
       "      <th>pred_time_val</th>\n",
       "      <th>fit_time</th>\n",
       "      <th>pred_time_val_marginal</th>\n",
       "      <th>fit_time_marginal</th>\n",
       "      <th>stack_level</th>\n",
       "      <th>can_infer</th>\n",
       "      <th>fit_order</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>WeightedEnsemble_L2</td>\n",
       "      <td>0.7034</td>\n",
       "      <td>2.267676</td>\n",
       "      <td>61.724523</td>\n",
       "      <td>0.009824</td>\n",
       "      <td>1.114678</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LightGBM_BAG_L1/T3</td>\n",
       "      <td>0.7030</td>\n",
       "      <td>0.748693</td>\n",
       "      <td>19.364498</td>\n",
       "      <td>0.748693</td>\n",
       "      <td>19.364498</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LightGBM_BAG_L1/T0</td>\n",
       "      <td>0.7012</td>\n",
       "      <td>0.753334</td>\n",
       "      <td>19.556658</td>\n",
       "      <td>0.753334</td>\n",
       "      <td>19.556658</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LightGBM_BAG_L2/T0</td>\n",
       "      <td>0.6994</td>\n",
       "      <td>6.867795</td>\n",
       "      <td>170.063830</td>\n",
       "      <td>0.732568</td>\n",
       "      <td>30.010442</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>WeightedEnsemble_L3</td>\n",
       "      <td>0.6994</td>\n",
       "      <td>6.877383</td>\n",
       "      <td>170.530808</td>\n",
       "      <td>0.009588</td>\n",
       "      <td>0.466978</td>\n",
       "      <td>3</td>\n",
       "      <td>True</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>LightGBM_BAG_L1/T1</td>\n",
       "      <td>0.6986</td>\n",
       "      <td>0.746327</td>\n",
       "      <td>20.226154</td>\n",
       "      <td>0.746327</td>\n",
       "      <td>20.226154</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>LightGBM_BAG_L1/T2</td>\n",
       "      <td>0.6976</td>\n",
       "      <td>0.762833</td>\n",
       "      <td>21.019193</td>\n",
       "      <td>0.762833</td>\n",
       "      <td>21.019193</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>NeuralNetMXNet_BAG_L2/T0</td>\n",
       "      <td>0.6480</td>\n",
       "      <td>9.412838</td>\n",
       "      <td>185.450613</td>\n",
       "      <td>3.277610</td>\n",
       "      <td>45.397226</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>NeuralNetMXNet_BAG_L1/T0</td>\n",
       "      <td>0.5954</td>\n",
       "      <td>3.124041</td>\n",
       "      <td>59.886885</td>\n",
       "      <td>3.124041</td>\n",
       "      <td>59.886885</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      model  score_val  pred_time_val    fit_time  \\\n",
       "0       WeightedEnsemble_L2     0.7034       2.267676   61.724523   \n",
       "1        LightGBM_BAG_L1/T3     0.7030       0.748693   19.364498   \n",
       "2        LightGBM_BAG_L1/T0     0.7012       0.753334   19.556658   \n",
       "3        LightGBM_BAG_L2/T0     0.6994       6.867795  170.063830   \n",
       "4       WeightedEnsemble_L3     0.6994       6.877383  170.530808   \n",
       "5        LightGBM_BAG_L1/T1     0.6986       0.746327   20.226154   \n",
       "6        LightGBM_BAG_L1/T2     0.6976       0.762833   21.019193   \n",
       "7  NeuralNetMXNet_BAG_L2/T0     0.6480       9.412838  185.450613   \n",
       "8  NeuralNetMXNet_BAG_L1/T0     0.5954       3.124041   59.886885   \n",
       "\n",
       "   pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  \\\n",
       "0                0.009824           1.114678            2       True   \n",
       "1                0.748693          19.364498            1       True   \n",
       "2                0.753334          19.556658            1       True   \n",
       "3                0.732568          30.010442            2       True   \n",
       "4                0.009588           0.466978            3       True   \n",
       "5                0.746327          20.226154            1       True   \n",
       "6                0.762833          21.019193            1       True   \n",
       "7                3.277610          45.397226            2       True   \n",
       "8                3.124041          59.886885            1       True   \n",
       "\n",
       "   fit_order  \n",
       "0          6  \n",
       "1          4  \n",
       "2          1  \n",
       "3          7  \n",
       "4          9  \n",
       "5          2  \n",
       "6          3  \n",
       "7          8  \n",
       "8          5  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predictor Leaderboard to comapre models\n",
    "predictor.leaderboard(silent=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2f78317",
   "metadata": {},
   "source": [
    "## <a id=\"Saving-and-Loading-Models\">Saving and Loading Models</a>\n",
    "\n",
    "AutoGluon automatically saves trained models to the disk. The default location is `/AutogluonModels/`. You can check the log to find the location, or get the path using `predictor.path`.\n",
    "\n",
    "You can also save the predictor using `predictor.save(path)`.\n",
    "\n",
    "You can not simply load a saved model from the file to obtain a trained predictor.\n",
    "\n",
    "\n",
    "(<a href=\"#0\">Go to top</a>)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "164a491c",
   "metadata": {},
   "source": [
    "Get the location of the trained models from the last training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5ca7bc06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'AutogluonModels/Tabular/'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor.path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f92b41fe",
   "metadata": {},
   "source": [
    "Since we have only considered a smaller subset of models for the previous `fit()` call, __let us load the models trained during our last hand-on notebook demo__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "60cd165e",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor_loaded = TabularPredictor.load('./AutogluonModels/Intro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0684f017",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Summary of fit() ***\n",
      "Estimated performance of each model:\n",
      "                  model  score_val  pred_time_val    fit_time  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n",
      "0   WeightedEnsemble_L2   0.769312       5.177530  794.675259                0.004050           1.241518            2       True         13\n",
      "1      RandomForestEntr   0.745503       0.466994   92.132452                0.466994          92.132452            1       True          6\n",
      "2              CatBoost   0.745503       1.036961  136.983842                1.036961         136.983842            1       True          7\n",
      "3            LightGBMXT   0.740741       0.382354   30.080697                0.382354          30.080697            1       True          3\n",
      "4      RandomForestGini   0.740741       0.544838   90.212166                0.544838          90.212166            1       True          5\n",
      "5              LightGBM   0.739153       0.459839   82.301038                0.459839          82.301038            1       True          4\n",
      "6         LightGBMLarge   0.737566       0.379415   49.950664                0.379415          49.950664            1       True         12\n",
      "7               XGBoost   0.731746       0.591545   57.927339                0.591545          57.927339            1       True         10\n",
      "8        ExtraTreesEntr   0.726984       0.577737  127.770066                0.577737         127.770066            1       True          9\n",
      "9        ExtraTreesGini   0.725397       0.522792  122.392076                0.522792         122.392076            1       True          8\n",
      "10       NeuralNetMXNet   0.703175       1.143607  309.370244                1.143607         309.370244            1       True         11\n",
      "11       KNeighborsDist   0.658730       0.209673    3.673903                0.209673           3.673903            1       True          2\n",
      "12       KNeighborsUnif   0.641270       0.211004    3.683402                0.211004           3.683402            1       True          1\n",
      "Number of models trained: 13\n",
      "Types of models trained:\n",
      "{'RFModel', 'KNNModel', 'XGBoostModel', 'WeightedEnsembleModel', 'XTModel', 'LGBModel', 'CatBoostModel', 'TabularNeuralNetModel'}\n",
      "Bagging used: False \n",
      "Multi-layer stack-ensembling used: False \n",
      "Feature Metadata (Processed):\n",
      "(raw dtype, special dtypes):\n",
      "('category', [])                    :    30 | ['product_type_1', 'brand_1', 'manufacturer_1', 'part_number_1', 'model_number_1', ...]\n",
      "('category', ['text_as_category'])  :     8 | ['item_name_1', 'product_description_1', 'bullet_point_1', 'generic_keyword_1', 'item_name_2', ...]\n",
      "('float', [])                       :    10 | ['normalized_item_weight_1', 'number_of_items_1', 'case_pack_quantity_1', 'item_package_quantity_1', 'normalized_item_package_weight_1', ...]\n",
      "('int', ['binned', 'text_special']) :   194 | ['item_name_1.char_count', 'item_name_1.word_count', 'item_name_1.capital_ratio', 'item_name_1.lower_ratio', 'item_name_1.digit_ratio', ...]\n",
      "('int', ['text_ngram'])             : 10001 | ['__nlp__.00', '__nlp__.000', '__nlp__.001', '__nlp__.01', '__nlp__.02', ...]\n",
      "Plot summary of models saved to file: ./AutogluonModels/Intro/SummaryOfModels.html\n",
      "*** End of fit() summary ***\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'model_types': {'KNeighborsUnif': 'KNNModel',\n",
       "  'KNeighborsDist': 'KNNModel',\n",
       "  'LightGBMXT': 'LGBModel',\n",
       "  'LightGBM': 'LGBModel',\n",
       "  'RandomForestGini': 'RFModel',\n",
       "  'RandomForestEntr': 'RFModel',\n",
       "  'CatBoost': 'CatBoostModel',\n",
       "  'ExtraTreesGini': 'XTModel',\n",
       "  'ExtraTreesEntr': 'XTModel',\n",
       "  'XGBoost': 'XGBoostModel',\n",
       "  'NeuralNetMXNet': 'TabularNeuralNetModel',\n",
       "  'LightGBMLarge': 'LGBModel',\n",
       "  'WeightedEnsemble_L2': 'WeightedEnsembleModel'},\n",
       " 'model_performance': {'KNeighborsUnif': 0.6412698412698413,\n",
       "  'KNeighborsDist': 0.6587301587301587,\n",
       "  'LightGBMXT': 0.7407407407407407,\n",
       "  'LightGBM': 0.7391534391534391,\n",
       "  'RandomForestGini': 0.7407407407407407,\n",
       "  'RandomForestEntr': 0.7455026455026456,\n",
       "  'CatBoost': 0.7455026455026456,\n",
       "  'ExtraTreesGini': 0.7253968253968254,\n",
       "  'ExtraTreesEntr': 0.726984126984127,\n",
       "  'XGBoost': 0.7317460317460317,\n",
       "  'NeuralNetMXNet': 0.7031746031746032,\n",
       "  'LightGBMLarge': 0.7375661375661375,\n",
       "  'WeightedEnsemble_L2': 0.7693121693121693},\n",
       " 'model_best': 'WeightedEnsemble_L2',\n",
       " 'model_paths': {'KNeighborsUnif': './AutogluonModels/Intro/models/KNeighborsUnif/',\n",
       "  'KNeighborsDist': './AutogluonModels/Intro/models/KNeighborsDist/',\n",
       "  'LightGBMXT': './AutogluonModels/Intro/models/LightGBMXT/',\n",
       "  'LightGBM': './AutogluonModels/Intro/models/LightGBM/',\n",
       "  'RandomForestGini': './AutogluonModels/Intro/models/RandomForestGini/',\n",
       "  'RandomForestEntr': './AutogluonModels/Intro/models/RandomForestEntr/',\n",
       "  'CatBoost': './AutogluonModels/Intro/models/CatBoost/',\n",
       "  'ExtraTreesGini': './AutogluonModels/Intro/models/ExtraTreesGini/',\n",
       "  'ExtraTreesEntr': './AutogluonModels/Intro/models/ExtraTreesEntr/',\n",
       "  'XGBoost': './AutogluonModels/Intro/models/XGBoost/',\n",
       "  'NeuralNetMXNet': './AutogluonModels/Intro/models/NeuralNetMXNet/',\n",
       "  'LightGBMLarge': './AutogluonModels/Intro/models/LightGBMLarge/',\n",
       "  'WeightedEnsemble_L2': './AutogluonModels/Intro/models/WeightedEnsemble_L2/'},\n",
       " 'model_fit_times': {'KNeighborsUnif': 3.6834020614624023,\n",
       "  'KNeighborsDist': 3.673903226852417,\n",
       "  'LightGBMXT': 30.08069658279419,\n",
       "  'LightGBM': 82.30103778839111,\n",
       "  'RandomForestGini': 90.21216559410095,\n",
       "  'RandomForestEntr': 92.13245248794556,\n",
       "  'CatBoost': 136.98384189605713,\n",
       "  'ExtraTreesGini': 122.39207649230957,\n",
       "  'ExtraTreesEntr': 127.77006578445435,\n",
       "  'XGBoost': 57.92733907699585,\n",
       "  'NeuralNetMXNet': 309.37024426460266,\n",
       "  'LightGBMLarge': 49.950663566589355,\n",
       "  'WeightedEnsemble_L2': 1.2415180206298828},\n",
       " 'model_pred_times': {'KNeighborsUnif': 0.21100378036499023,\n",
       "  'KNeighborsDist': 0.2096726894378662,\n",
       "  'LightGBMXT': 0.3823537826538086,\n",
       "  'LightGBM': 0.4598388671875,\n",
       "  'RandomForestGini': 0.5448377132415771,\n",
       "  'RandomForestEntr': 0.4669942855834961,\n",
       "  'CatBoost': 1.036961317062378,\n",
       "  'ExtraTreesGini': 0.5227921009063721,\n",
       "  'ExtraTreesEntr': 0.5777373313903809,\n",
       "  'XGBoost': 0.5915453433990479,\n",
       "  'NeuralNetMXNet': 1.1436066627502441,\n",
       "  'LightGBMLarge': 0.37941527366638184,\n",
       "  'WeightedEnsemble_L2': 0.004049777984619141},\n",
       " 'num_bag_folds': 0,\n",
       " 'max_stack_level': 2,\n",
       " 'num_classes': 2,\n",
       " 'model_hyperparams': {'KNeighborsUnif': {'weights': 'uniform', 'n_jobs': -1},\n",
       "  'KNeighborsDist': {'weights': 'distance', 'n_jobs': -1},\n",
       "  'LightGBMXT': {'num_boost_round': 10000,\n",
       "   'num_threads': -1,\n",
       "   'learning_rate': 0.05,\n",
       "   'objective': 'binary',\n",
       "   'verbose': -1,\n",
       "   'boosting_type': 'gbdt',\n",
       "   'two_round': True,\n",
       "   'extra_trees': True},\n",
       "  'LightGBM': {'num_boost_round': 10000,\n",
       "   'num_threads': -1,\n",
       "   'learning_rate': 0.05,\n",
       "   'objective': 'binary',\n",
       "   'verbose': -1,\n",
       "   'boosting_type': 'gbdt',\n",
       "   'two_round': True},\n",
       "  'RandomForestGini': {'n_estimators': 300,\n",
       "   'n_jobs': -1,\n",
       "   'random_state': 0,\n",
       "   'bootstrap': True,\n",
       "   'criterion': 'gini'},\n",
       "  'RandomForestEntr': {'n_estimators': 300,\n",
       "   'n_jobs': -1,\n",
       "   'random_state': 0,\n",
       "   'bootstrap': True,\n",
       "   'criterion': 'entropy'},\n",
       "  'CatBoost': {'iterations': 10000,\n",
       "   'learning_rate': 0.05,\n",
       "   'random_seed': 0,\n",
       "   'allow_writing_files': False,\n",
       "   'eval_metric': 'Accuracy'},\n",
       "  'ExtraTreesGini': {'n_estimators': 300,\n",
       "   'n_jobs': -1,\n",
       "   'random_state': 0,\n",
       "   'bootstrap': True,\n",
       "   'criterion': 'gini'},\n",
       "  'ExtraTreesEntr': {'n_estimators': 300,\n",
       "   'n_jobs': -1,\n",
       "   'random_state': 0,\n",
       "   'bootstrap': True,\n",
       "   'criterion': 'entropy'},\n",
       "  'XGBoost': {'n_estimators': 10000,\n",
       "   'learning_rate': 0.1,\n",
       "   'n_jobs': -1,\n",
       "   'proc.max_category_levels': 100,\n",
       "   'objective': 'binary:logistic',\n",
       "   'booster': 'gbtree',\n",
       "   'use_label_encoder': False},\n",
       "  'NeuralNetMXNet': {'num_epochs': 500,\n",
       "   'epochs_wo_improve': 20,\n",
       "   'seed_value': None,\n",
       "   'proc.embed_min_categories': 4,\n",
       "   'proc.impute_strategy': 'median',\n",
       "   'proc.max_category_levels': 100,\n",
       "   'proc.skew_threshold': 0.99,\n",
       "   'network_type': 'widedeep',\n",
       "   'layers': None,\n",
       "   'numeric_embed_dim': None,\n",
       "   'activation': 'relu',\n",
       "   'max_layer_width': 2056,\n",
       "   'embedding_size_factor': 1.0,\n",
       "   'embed_exponent': 0.56,\n",
       "   'max_embedding_dim': 100,\n",
       "   'y_range': None,\n",
       "   'y_range_extend': 0.05,\n",
       "   'use_batchnorm': True,\n",
       "   'dropout_prob': 0.1,\n",
       "   'batch_size': 512,\n",
       "   'loss_function': None,\n",
       "   'optimizer': 'adam',\n",
       "   'learning_rate': 0.0003,\n",
       "   'weight_decay': 1e-06,\n",
       "   'clip_gradient': 100.0,\n",
       "   'momentum': 0.9,\n",
       "   'lr_scheduler': None,\n",
       "   'base_lr': 3e-05,\n",
       "   'target_lr': 1.0,\n",
       "   'lr_decay': 0.1,\n",
       "   'warmup_epochs': 10,\n",
       "   'use_ngram_features': False},\n",
       "  'LightGBMLarge': {'num_boost_round': 10000,\n",
       "   'num_threads': -1,\n",
       "   'learning_rate': 0.03,\n",
       "   'objective': 'binary',\n",
       "   'verbose': -1,\n",
       "   'boosting_type': 'gbdt',\n",
       "   'two_round': True,\n",
       "   'num_leaves': 128,\n",
       "   'feature_fraction': 0.9,\n",
       "   'min_data_in_leaf': 5},\n",
       "  'WeightedEnsemble_L2': {'use_orig_features': False,\n",
       "   'max_base_models': 25,\n",
       "   'max_base_models_per_type': 5,\n",
       "   'save_bag_folds': True}},\n",
       " 'leaderboard':                   model  score_val  pred_time_val    fit_time  \\\n",
       " 0   WeightedEnsemble_L2   0.769312       5.177530  794.675259   \n",
       " 1      RandomForestEntr   0.745503       0.466994   92.132452   \n",
       " 2              CatBoost   0.745503       1.036961  136.983842   \n",
       " 3            LightGBMXT   0.740741       0.382354   30.080697   \n",
       " 4      RandomForestGini   0.740741       0.544838   90.212166   \n",
       " 5              LightGBM   0.739153       0.459839   82.301038   \n",
       " 6         LightGBMLarge   0.737566       0.379415   49.950664   \n",
       " 7               XGBoost   0.731746       0.591545   57.927339   \n",
       " 8        ExtraTreesEntr   0.726984       0.577737  127.770066   \n",
       " 9        ExtraTreesGini   0.725397       0.522792  122.392076   \n",
       " 10       NeuralNetMXNet   0.703175       1.143607  309.370244   \n",
       " 11       KNeighborsDist   0.658730       0.209673    3.673903   \n",
       " 12       KNeighborsUnif   0.641270       0.211004    3.683402   \n",
       " \n",
       "     pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  \\\n",
       " 0                 0.004050           1.241518            2       True   \n",
       " 1                 0.466994          92.132452            1       True   \n",
       " 2                 1.036961         136.983842            1       True   \n",
       " 3                 0.382354          30.080697            1       True   \n",
       " 4                 0.544838          90.212166            1       True   \n",
       " 5                 0.459839          82.301038            1       True   \n",
       " 6                 0.379415          49.950664            1       True   \n",
       " 7                 0.591545          57.927339            1       True   \n",
       " 8                 0.577737         127.770066            1       True   \n",
       " 9                 0.522792         122.392076            1       True   \n",
       " 10                1.143607         309.370244            1       True   \n",
       " 11                0.209673           3.673903            1       True   \n",
       " 12                0.211004           3.683402            1       True   \n",
       " \n",
       "     fit_order  \n",
       " 0          13  \n",
       " 1           6  \n",
       " 2           7  \n",
       " 3           3  \n",
       " 4           5  \n",
       " 5           4  \n",
       " 6          12  \n",
       " 7          10  \n",
       " 8           9  \n",
       " 9           8  \n",
       " 10         11  \n",
       " 11          2  \n",
       " 12          1  }"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor_loaded.fit_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b5b3741",
   "metadata": {},
   "source": [
    "## <a id=\"Model-Inference\">Model Inference</a>\n",
    "\n",
    "\n",
    "\n",
    "(<a href=\"#0\">Go to top</a>)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b27b914",
   "metadata": {},
   "source": [
    "We can make a prediction on an individual example rather than on a full dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8389d8da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10    0\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Select one datapoint to make a prediction\n",
    "datapoint = df_test.iloc[[10]] # Note: .iloc[0] won't work because it returns pandas Series instead of DataFrame\n",
    "\n",
    "predictor_loaded.predict(datapoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "399598a7",
   "metadata": {},
   "source": [
    "To output predicted class probabilities instead of predicted classes, you can use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "abd23e03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.754881</td>\n",
       "      <td>0.245119</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           0         1\n",
       "10  0.754881  0.245119"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Returns a DataFrame that shows which probability corresponds to which class\n",
    "predictor_loaded.predict_proba(datapoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe1616e4",
   "metadata": {},
   "source": [
    "By default, `predict()` and `predict_proba()` will utilize the model that AutoGluon thinks is most accurate, which is usually an ensemble of many individual models. Here’s how to see which model this corresponds to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "71d05f80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'WeightedEnsemble_L2'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor_loaded.get_model_best()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07977002",
   "metadata": {},
   "source": [
    "### Selecting individual models for predictions\n",
    "We can specify a particular model to use for predictions (e.g. to reduce inference latency). Note that a ‘model’ in AutoGluon may refer to for example a single Neural Network, a bagged ensemble of many Neural Network copies trained on different training/validation splits, a weighted ensemble that aggregates the predictions of many other models, or a stacked model that operates on predictions output by other models. This is akin to viewing a RandomForest as one ‘model’ when it is in fact an ensemble of many decision trees.\n",
    "\n",
    "\n",
    "Here’s how to specify a particular model to use for prediction instead of AutoGluon’s default model-choice:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e9e18a6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction from KNeighborsUnif model: 0\n"
     ]
    }
   ],
   "source": [
    "# index of model to use\n",
    "i = 0\n",
    "model_to_use = predictor_loaded.get_model_names()[i]\n",
    "model_pred = predictor_loaded.predict(datapoint, model=model_to_use)\n",
    "print(f\"Prediction from {model_to_use} model: {model_pred.iloc[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6758a0fa",
   "metadata": {},
   "source": [
    "We can easily access information about the trained predictor or a particular model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7c87d7b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_models = predictor_loaded.get_model_names()\n",
    "model_to_use = all_models[i]\n",
    "specific_model = predictor_loaded._trainer.load_model(model_to_use)\n",
    "\n",
    "# Objects defined below are dicts with information (not printed here as they are quite large):\n",
    "model_info = specific_model.get_info()\n",
    "predictor_information = predictor_loaded.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "63bb9f4d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'KNeighborsUnif',\n",
       " 'model_type': 'KNNModel',\n",
       " 'problem_type': 'binary',\n",
       " 'eval_metric': 'accuracy',\n",
       " 'stopping_metric': 'accuracy',\n",
       " 'fit_time': 3.6834020614624023,\n",
       " 'num_classes': 2,\n",
       " 'quantile_levels': None,\n",
       " 'predict_time': 0.21100378036499023,\n",
       " 'val_score': 0.6412698412698413,\n",
       " 'hyperparameters': {'weights': 'uniform', 'n_jobs': -1},\n",
       " 'hyperparameters_fit': {},\n",
       " 'hyperparameters_nondefault': ['weights'],\n",
       " 'ag_args_fit': {'max_memory_usage_ratio': 1.0,\n",
       "  'max_time_limit_ratio': 1.0,\n",
       "  'max_time_limit': None,\n",
       "  'min_time_limit': 0,\n",
       "  'ignored_type_group_special': ['bool',\n",
       "   'text_ngram',\n",
       "   'text_special',\n",
       "   'datetime_as_int'],\n",
       "  'ignored_type_group_raw': ['bool', 'category', 'object'],\n",
       "  'get_features_kwargs': None,\n",
       "  'get_features_kwargs_extra': None},\n",
       " 'num_features': 10,\n",
       " 'features': ['normalized_item_weight_1',\n",
       "  'number_of_items_1',\n",
       "  'case_pack_quantity_1',\n",
       "  'item_package_quantity_1',\n",
       "  'normalized_item_package_weight_1',\n",
       "  'normalized_item_weight_2',\n",
       "  'number_of_items_2',\n",
       "  'case_pack_quantity_2',\n",
       "  'item_package_quantity_2',\n",
       "  'normalized_item_package_weight_2'],\n",
       " 'feature_metadata': <autogluon.core.features.feature_metadata.FeatureMetadata at 0x7fb6a17bd0f0>,\n",
       " 'memory_size': 2511930}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98b2770e",
   "metadata": {},
   "source": [
    "# <a id=\"Part-II---Additional-Features\">Part II - Additional Features</a>\n",
    "\n",
    "(<a href=\"#0\">Go to top</a>)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "294b3a22",
   "metadata": {},
   "source": [
    "## <a id=\"Feature-Importance\">Feature Importance</a>\n",
    "\n",
    "To better understand our trained predictor, we can estimate the overall importance of each feature:\n",
    "\n",
    "(<a href=\"#0\">Go to top</a>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9e4a2a2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing feature importance via permutation shuffling for 63 features using 1000 rows with 3 shuffle sets...\n",
      "\t1025.81s\t= Expected runtime (341.94s per shuffle set)\n",
      "\t526.0s\t= Actual runtime (Completed 3 of 3 shuffle sets)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>importance</th>\n",
       "      <th>stddev</th>\n",
       "      <th>p_value</th>\n",
       "      <th>n</th>\n",
       "      <th>p99_high</th>\n",
       "      <th>p99_low</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>product_description_1</th>\n",
       "      <td>0.127000</td>\n",
       "      <td>0.002000</td>\n",
       "      <td>0.000041</td>\n",
       "      <td>3</td>\n",
       "      <td>0.138460</td>\n",
       "      <td>0.115540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>product_description_2</th>\n",
       "      <td>0.116333</td>\n",
       "      <td>0.006506</td>\n",
       "      <td>0.000521</td>\n",
       "      <td>3</td>\n",
       "      <td>0.153616</td>\n",
       "      <td>0.079051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>generic_keyword_1</th>\n",
       "      <td>0.050333</td>\n",
       "      <td>0.005686</td>\n",
       "      <td>0.002114</td>\n",
       "      <td>3</td>\n",
       "      <td>0.082916</td>\n",
       "      <td>0.017751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bullet_point_2</th>\n",
       "      <td>0.042000</td>\n",
       "      <td>0.009849</td>\n",
       "      <td>0.008920</td>\n",
       "      <td>3</td>\n",
       "      <td>0.098435</td>\n",
       "      <td>-0.014435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>item_name_1</th>\n",
       "      <td>0.036667</td>\n",
       "      <td>0.001528</td>\n",
       "      <td>0.000289</td>\n",
       "      <td>3</td>\n",
       "      <td>0.045420</td>\n",
       "      <td>0.027914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>style_2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>3</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lifestyle_2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>3</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>normalized_item_weight_2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>3</td>\n",
       "      <td>0.005730</td>\n",
       "      <td>-0.005730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>color_2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>3</td>\n",
       "      <td>0.005730</td>\n",
       "      <td>-0.005730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>list_price_value_1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>3</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>63 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                          importance    stddev   p_value  n  p99_high  \\\n",
       "product_description_1       0.127000  0.002000  0.000041  3  0.138460   \n",
       "product_description_2       0.116333  0.006506  0.000521  3  0.153616   \n",
       "generic_keyword_1           0.050333  0.005686  0.002114  3  0.082916   \n",
       "bullet_point_2              0.042000  0.009849  0.008920  3  0.098435   \n",
       "item_name_1                 0.036667  0.001528  0.000289  3  0.045420   \n",
       "...                              ...       ...       ... ..       ...   \n",
       "style_2                     0.000000  0.000000  0.500000  3  0.000000   \n",
       "lifestyle_2                 0.000000  0.000000  0.500000  3  0.000000   \n",
       "normalized_item_weight_2    0.000000  0.001000  0.500000  3  0.005730   \n",
       "color_2                     0.000000  0.001000  0.500000  3  0.005730   \n",
       "list_price_value_1          0.000000  0.000000  0.500000  3  0.000000   \n",
       "\n",
       "                           p99_low  \n",
       "product_description_1     0.115540  \n",
       "product_description_2     0.079051  \n",
       "generic_keyword_1         0.017751  \n",
       "bullet_point_2           -0.014435  \n",
       "item_name_1               0.027914  \n",
       "...                            ...  \n",
       "style_2                   0.000000  \n",
       "lifestyle_2               0.000000  \n",
       "normalized_item_weight_2 -0.005730  \n",
       "color_2                  -0.005730  \n",
       "list_price_value_1        0.000000  \n",
       "\n",
       "[63 rows x 6 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor_loaded.feature_importance(df_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e4a527e",
   "metadata": {},
   "source": [
    "Computed via permutation-shuffling, these feature importance scores quantify the drop in predictive performance (of the already trained predictor) when one columns values are randomly shuffled across rows. The top features in this list contribute most to AutoGluon’s accuracy. Features with non-positive importance score hardly contribute to the predictors accuracy, or may even be actively harmful to include in the data (consider removing these features from your data and calling `fit` again). These scores facilitate interpretability of the predictors global behavior (which features it relies on for all predictions) rather than local explanations that only rationalize one particular prediction.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "526652f9",
   "metadata": {},
   "source": [
    "## <a id=\"Inference-Speed\">Inference Speed</a>\n",
    "\n",
    "While computationally-favorable, single individual models will usually have lower accuracy than weighted/stacked/bagged ensembles. However, training and inference time often play a significant role in model evaluation. Models which yield the highest accuracy, may not be most suitable for real world scenarios if their inference speeds are too low. AutoGluon offers a few solutions to boost inference speeds without compromising too much on performance.\n",
    "- Persisting models in memory\n",
    "- Distillation\n",
    "- Using presets during `fit()`\n",
    "\n",
    "(<a href=\"#0\">Go to top</a>)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce8f700f",
   "metadata": {},
   "source": [
    "### Persisting models in memory\n",
    "AutoGluon can force models to persist in memory instead of reading them from disk each time. This solution may consume more memory to maintain all models in memory.\n",
    "\n",
    "(<a href=\"#0\">Go to top</a>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1ca4a399",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Persisting 11 models in memory. Models will require 1.06% of memory.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['LightGBMXT',\n",
       " 'CatBoost',\n",
       " 'LightGBM',\n",
       " 'WeightedEnsemble_L2',\n",
       " 'LightGBMLarge',\n",
       " 'ExtraTreesGini',\n",
       " 'RandomForestGini',\n",
       " 'KNeighborsUnif',\n",
       " 'RandomForestEntr',\n",
       " 'XGBoost',\n",
       " 'ExtraTreesEntr']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Force models to persist in memory\n",
    "predictor_loaded.persist_models()\n",
    "\n",
    "# List models which are persisting in memory\n",
    "predictor_loaded.get_model_names_persisted()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1db8b43f",
   "metadata": {},
   "source": [
    "### Distillation\n",
    "Model Distillation offers one way to retain the computational benefits of a single model, while enjoying some of the accuracy-boost that comes with ensembling. The idea is to train the individual model (which we can call the student) to mimic the predictions of the full stack ensemble (the teacher). Like `refit_full()`, the `distill()` function will produce additional models we can opt to use for prediction.\n",
    "\n",
    "(<a href=\"#0\">Go to top</a>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b85138b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify much longer time limit in real applications\n",
    "student_models = predictor.distill(time_limit=10*30)\n",
    "student_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0209a6fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_student = predictor.predict(df_test, model=student_models[0])\n",
    "print(f\"predictions from {student_models[0]}: {list(preds_student)[:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f675157",
   "metadata": {},
   "source": [
    "## <a id=\"Excluding-Models\">Excluding Models</a>\n",
    "\n",
    "Finally, you may also exclude specific unwieldy models from being trained at all. Below we exclude models that tend to be slower (K Nearest Neighbors, Neural Network, models with custom larger-than-default hyperparameters):\n",
    "\n",
    "(<a href=\"#0\">Go to top</a>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf13f195",
   "metadata": {},
   "outputs": [],
   "source": [
    "excluded_model_types = [\"KNN\", \"NN\", \"custom\"]\n",
    "predictor_light = TabularPredictor(label=\"occupation\", eval_metric=metric).fit(\n",
    "    train_data, excluded_model_types=excluded_model_types, time_limit=30\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c73f79",
   "metadata": {},
   "source": [
    "## <a name=\"Cleaning-up-Model-Artifacts\">Cleaning up Model Artifacts</a>\n",
    "\n",
    "After you are done with this Demo, clean model artifacts by uncommenting and executing the cell below.\n",
    "\n",
    "__It is always good practice to clean everything when you are done, preventing the disk from getting full.__\n",
    "\n",
    "(<a href=\"#0\">Go to top</a>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c6a6db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !rm -r AutogluonModels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed36edd1",
   "metadata": {},
   "source": [
    "<p style=\"padding: 10px; border: 1px solid black;\">\n",
    "<img src=\"./utils/MLU-NEW-logo.png\" alt=\"drawing\" width=\"400\"/> <br/>\n",
    "\n",
    "# Thank you!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_amazonei_mxnet_p36",
   "language": "python",
   "name": "conda_amazonei_mxnet_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
